{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil as sh\n",
    "import os\n",
    "import glob as gb\n",
    "import re\n",
    "\n",
    "\n",
    "## For preprocessing\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from gensim.parsing.preprocessing import stem_text\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "from gensim.parsing.preprocessing import strip_short\n",
    "from gensim import matutils, models\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import words\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('words')\n",
    "\n",
    "import scipy.sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\padu\\\\Desktop\\\\Zillow\\\\ZillowCraiglist\"\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Zillow = pd.read_csv(\"FinalZillowCraig.csv\")\n",
    "Zillow = pd.read_csv(\"FinalZillowStreetAddress.csv\")\n",
    "# Zillow = pd.read_csv(\"FinalZillowOverview.csv\")\n",
    "# Zillow = pd.read_csv(\"FinalStreetOverview.csv\")\n",
    "                     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview = Zillow['overview']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview = [text.split() for text in overview]\n",
    "overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in overview:\n",
    "    for col in row:\n",
    "        if col.endswith('.com'):\n",
    "            print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview = [strip_punctuation(text) for text in overview]\n",
    "overview = [strip_multiple_whitespaces(text) for text in overview]\n",
    "overview = [strip_numeric(text) for text in overview]\n",
    "overview = [remove_stopwords(text) for text in overview]\n",
    "overview = [strip_short(text,2) for text in overview]\n",
    "overview = [strip_short(text,1) for text in overview]\n",
    "overview = [text.lower() for text in overview]\n",
    "overview = [re.sub(r\"\\S*https?:\\S*\", \"\", text) for text in overview]\n",
    "overview = [re.sub(r\"(繁體中文,日本語)\",\"\", text ) for text in overview]\n",
    "overview = [text.replace(\"arwen\",\"\") for text in overview]\n",
    "overview = [text.replace(\"apartments\",\"apartment\") for text in overview]\n",
    "overview = [text.replace(\"vista\",\"\") for text in overview]\n",
    "overview = [text.replace(\"home\",\"\") for text in overview]\n",
    "# overview = [text.replace(\"com\",\"\") for text in overview]\n",
    "overview = [re.sub(r'[^\\x00-\\x7F]+',' ', text) for text in overview]\n",
    "overview = [re.sub(r'[^A-Za-z]+',' ', text) for text in overview]\n",
    "# overview = [nltk.word_tokenize(text) for text in overview]\n",
    "# overview = [strip_short(text,2) for text in overview]\n",
    "# overview = [strip_short(text,1) for text in overview]\n",
    "# overview = [remove_stopwords(text) for text in overview]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zillow['Clean'] = overview  ## Map the cleaned data to the original dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count word occurrences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##  append cleaned data to tokenlist \n",
    "\n",
    "tokenlist = []\n",
    "\n",
    "for item in overview:\n",
    "    tokenlist.append(item)\n",
    "    \n",
    "tokenlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert tokenlist to a string \n",
    "\n",
    "String = \"\"\n",
    "String = String.join(tokenlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in String:\n",
    "    print(row, end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized = nltk.word_tokenize(String)\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in tokenized:\n",
    "    if row.endswith('.com'):\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [row for row in tokenized if row!= row.endswith('.com')]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in tokenized:\n",
    "    a = row.endswith(\".com\")\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in tokenized:\n",
    "    for col in row:\n",
    "        print(col, end = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## This code checks for non-english words and removes them\n",
    "# String\n",
    "# \" \".join(w for w in nltk.wordpunct_tokenize(String) \\\n",
    "#          if w.lower() in words or not w.isalpha())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ This code tokenizes the string test created \n",
    "\n",
    "tokenized = nltk.word_tokenize(String)\n",
    "tokenized = [remove_stopwords(text) for text in tokenized]\n",
    "\n",
    "\n",
    "## This code counts the freqency of works in the tokenzided strings the plot it\n",
    "tokenplot = nltk.FreqDist(tokenized)\n",
    "tokenplot.plot(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized\n",
    "\n",
    "# for word in tokenlist:\n",
    "#     if word in words.words():\n",
    "#         Tokens.append(word)\n",
    "        \n",
    "# tokenized\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Vectorize = CountVectorizer(stop_words=\"english\")\n",
    "DataVectorize = Vectorize.fit_transform(Zillow.Clean)\n",
    "Data = pd.DataFrame(DataVectorize.toarray(), columns = Vectorize.get_feature_names())\n",
    "Data.index = Zillow.index\n",
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Giberish = []\n",
    "\n",
    "for words in Data.columns:\n",
    "    Giberish.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Giberish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Realwords = []\n",
    "\n",
    "for word in Giberish:\n",
    "    if word in words.words():\n",
    "        Realwords.append(word)\n",
    "        \n",
    "Realwords\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VectorizedString = \"\"\n",
    "VectorizedString = String.join(Realwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ This code tokenizes the string test created \n",
    "\n",
    "Vectortokenized = nltk.word_tokenize(VectorizedString)\n",
    "Vectortokenized = [remove_stopwords(text) for text in tokenized]\n",
    "\n",
    "## This code counts the freqency of works in the tokenzided strings the plot it\n",
    "Vtokenplot = nltk.FreqDist(Vectortokenized )\n",
    "Vtokenplot.plot(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Restricted = ['income', 'background', 'criminal', 'eviction', 'credit', 'score', 'history',\n",
    "              'proof', 'voucher', 'unemployment', 'evictions', 'felony', 'report', 'felonies',\n",
    "              'hoa','section', \"arrests\"]\n",
    "\n",
    "Restriction = \" \".join(Restricted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Restriction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in Restricted:\n",
    "    Zillow[str(col)] = [words.count(col) for words in Zillow['overview']]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zillow['Restrictions']=Zillow.loc[:,Restricted].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zillow.to_csv(\"Restrictions.csv\", index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for words in Zillow[\"overview\"]:\n",
    "    if words in Restriction:\n",
    "        print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for words in Zillow[\"overview\"]:\n",
    "    print(words.count('Background'))\n",
    "        print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
